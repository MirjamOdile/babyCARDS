{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random held-out testing dataset (15%) of each side to measure performance on unseen twitter users\n",
    "random.seed(1)\n",
    "samplesize_denier = int(round(0.15*len(os.listdir('data/tweets/contrarian')),0))\n",
    "test_denier = random.sample(range(len(os.listdir('data/tweets/contrarian'))), samplesize_denier)\n",
    "train_denier = [i for i in range(len(os.listdir('data/tweets/contrarian'))) if i not in test_denier]\n",
    "\n",
    "samplesize_pro = int(round(0.15*len(os.listdir('data/tweets/convinced')),0))\n",
    "test_pro = random.sample(range(len(os.listdir('data/tweets/convinced'))), samplesize_pro)  # Pick 4 random items from the list\n",
    "train_pro = [i for i in range(len(os.listdir('data/tweets/convinced'))) if i not in test_pro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load training/validation data: contrarian tweets---\n",
      "\n",
      "0 PMgeezer.json\n",
      "1 FreedomWorks.json\n",
      "2 FoF_Liberty.json\n",
      "3 JamesDelingpole.json\n",
      "4 ellymelly.json\n",
      "5 EcoSenseNow.json\n",
      "6 ACSHorg.json\n",
      "7 HeartlandInst.json\n",
      "8 CFACT.json\n",
      "9 FoxNews.json\n",
      "10 ReasonFdn.json\n",
      "11 EnergyBrief.json\n",
      "12 RogerPielkeJr.json\n",
      "13 FriendsOScience.json\n",
      "14 HudsonInstitute.json\n",
      "15 velardedaoiz2.json\n",
      "16 DailySignal.json\n",
      "17 wattsupwiththat.json\n",
      "18 WAPolicyCenter.json\n",
      "19 IERenergy.json\n",
      "20 HaveWeAllGoneM1.json\n",
      "21 tan123.json\n",
      "22 catoletters.json\n",
      "23 AlexEpstein.json\n",
      "24 StopTheseThings.json\n",
      "25 curryja.json\n",
      "26 ShellenbergerMD.json\n",
      "27 ClimateAudit.json\n",
      "28 ronnieressler.json\n",
      "29 JunkScience.json\n",
      "30 JaggerMickOZ.json\n",
      "31 Toimatom.json\n",
      "32 BreitbartNews.json\n",
      "33 DavidRoseUK.json\n",
      "34 BjornLomborg.json\n",
      "35 rmack2x.json\n",
      "36 ceidotorg.json\n",
      "37 PacificResearch.json\n",
      "38 IvoVegter.json\n",
      "39 CatoInstitute.json\n",
      "40 Heritage.json\n",
      "41 nigella_i5e.json\n",
      "42 AboutFreedom999.json\n",
      "43 ManhattanInst.json\n",
      "44 Tony__Heller.json\n",
      "45 AEI.json\n",
      "46 FraserInstitute.json\n",
      "47 ClimateDepot.json\n",
      "48 szabosolicitors.json\n",
      "49 capitalresearch.json\n",
      "\n",
      " 1734757 contrarian tweets loaded.\n",
      "\n",
      "\n",
      "---Load training/validation data: convinced tweets---\n",
      "\n",
      "0 KHayhoe.json\n",
      "1 ConversationEDU.json\n",
      "2 ClimateOfGavin.json\n",
      "3 algore.json\n",
      "4 NaomiAKlein.json\n",
      "5 GretaThunberg.json\n",
      "6 HuffPostGreen.json\n",
      "7 climate.json\n",
      "8 UNEP.json\n",
      "9 mzjacobson.json\n",
      "10 grist.json\n",
      "11 PlanetGreen.json\n",
      "12 ed_hawkins.json\n",
      "13 oxfamgb.json\n",
      "14 ClimateTracking.json\n",
      "15 washingtonpost.json\n",
      "16 UCSUSA.json\n",
      "17 climatecouncil.json\n",
      "18 DrShepherd2013.json\n",
      "19 friends_earth.json\n",
      "20 billmckibben.json\n",
      "21 guardian.json\n",
      "22 UNFCCC.json\n",
      "23 the_ecologist.json\n",
      "24 ayanaeliza.json\n",
      "25 DrKateMarvel.json\n",
      "26 climateWWF.json\n",
      "27 coralsncaves.json\n",
      "28 CarbonBrief.json\n",
      "29 DoctorVive.json\n",
      "30 sunrisemvmt.json\n",
      "31 nature_org.json\n",
      "32 pewenvironment.json\n",
      "33 EnvDefenseFund.json\n",
      "34 globalactplan.json\n",
      "35 MichaelEMann.json\n",
      "36 Treehugger.json\n",
      "\n",
      " 1431443 convinced tweets loaded.\n",
      "\n",
      "\n",
      "Training data set with 2374650 tweets created.\n",
      "Validation data set with 791550 tweets created.\n"
     ]
    }
   ],
   "source": [
    "# Create the training and validation data sets\n",
    "print('---Load training/validation data: contrarian tweets---\\n')\n",
    "tweets_contrarian = []\n",
    "username_contrarian = []\n",
    "name_contrarian = []\n",
    "date_contrarian = []\n",
    "time_contrarian = []\n",
    "for i, file in enumerate([os.listdir('data/tweets/contrarian')[j] for j in train_denier]):\n",
    "    print(i, file)\n",
    "    for line in open(''.join(['data/tweets/contrarian/', file]), 'r'):\n",
    "        tweets_contrarian.append(json.loads(line)['tweet'])\n",
    "        username_contrarian.append(json.loads(line)['username'])\n",
    "        name_contrarian.append(json.loads(line)['name'])\n",
    "        date_contrarian.append(json.loads(line)['date'])\n",
    "        time_contrarian.append(json.loads(line)['time'])\n",
    "print(\"\\n\", len(tweets_contrarian), \"contrarian tweets loaded.\")\n",
    "\n",
    "print('\\n\\n---Load training/validation data: convinced tweets---\\n')\n",
    "tweets_convinced = []\n",
    "username_convinced = []\n",
    "name_convinced = []\n",
    "date_convinced = []\n",
    "time_convinced = []\n",
    "for i, file in enumerate([os.listdir('data/tweets/convinced')[j] for j in train_pro]):\n",
    "    print(i, file)\n",
    "    for line in open(''.join(['data/tweets/convinced/', file]), 'r'):\n",
    "        tweets_convinced.append(json.loads(line)['tweet'])\n",
    "        username_convinced.append(json.loads(line)['username'])\n",
    "        name_convinced.append(json.loads(line)['name'])\n",
    "        date_convinced.append(json.loads(line)['date'])\n",
    "        time_convinced.append(json.loads(line)['time'])\n",
    "print(\"\\n\", len(tweets_convinced), \"convinced tweets loaded.\")        \n",
    "        \n",
    "df = pd.DataFrame(list(zip(\n",
    "    tweets_contrarian + tweets_convinced, \n",
    "    [1]*len(tweets_contrarian) + [0]*len(tweets_convinced),\n",
    "    username_contrarian + username_convinced,\n",
    "    name_contrarian + name_convinced,\n",
    "    date_contrarian + date_convinced,\n",
    "    time_contrarian + time_convinced)),\n",
    "                    columns = ['text', 'label', 'username', 'name', 'date', 'time'])\n",
    "\n",
    "# Split training data into training and validation dataset\n",
    "train, valid = train_test_split(df, test_size=0.25, random_state=1, shuffle=True)\n",
    "print(\"\\n\\nTraining data set with {} tweets created.\".format(len(train)))\n",
    "print(\"Validation data set with {} tweets created.\".format(len(valid)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load independent testing data: contrarian tweets---\n",
      "\n",
      "0 ActivistPost.json\n",
      "1 RyanMaue.json\n",
      "2 mattwridley.json\n",
      "3 ElianaBenador.json\n",
      "4 SeibtNaomi.json\n",
      "5 EnergyCitizens.json\n",
      "6 Haggisman57.json\n",
      "7 NationalCenter.json\n",
      "8 ClimateRealists.json\n",
      "\n",
      " 231942 contrarian tweets loaded.\n",
      "\n",
      "\n",
      "---Load independent testing data: convinced tweets---\n",
      "\n",
      "0 IPCC_CH.json\n",
      "1 GreenpeaceUK.json\n",
      "2 ClimateCentral.json\n",
      "3 NASAClimate.json\n",
      "4 ProjectDrawdown.json\n",
      "5 ClimatePower.json\n",
      "\n",
      " 142178 convinced tweets loaded.\n",
      "\n",
      "\n",
      "Testing data set with 374120 tweets created.\n"
     ]
    }
   ],
   "source": [
    "# Create an independent testing data set\n",
    "print('---Load independent testing data: contrarian tweets---\\n')\n",
    "tweets_contrarian = []\n",
    "username_contrarian = []\n",
    "name_contrarian = []\n",
    "date_contrarian = []\n",
    "time_contrarian = []\n",
    "for i, file in enumerate([os.listdir('data/tweets/contrarian')[j] for j in test_denier]):\n",
    "    print(i, file)\n",
    "    for line in open(''.join(['data/tweets/contrarian/', file]), 'r'):\n",
    "        tweets_contrarian.append(json.loads(line)['tweet'])\n",
    "        username_contrarian.append(json.loads(line)['username'])\n",
    "        name_contrarian.append(json.loads(line)['name'])\n",
    "        date_contrarian.append(json.loads(line)['date'])\n",
    "        time_contrarian.append(json.loads(line)['time'])\n",
    "print(\"\\n\", len(tweets_contrarian), \"contrarian tweets loaded.\")\n",
    "\n",
    "print('\\n\\n---Load independent testing data: convinced tweets---\\n')\n",
    "tweets_convinced = []\n",
    "username_convinced = []\n",
    "name_convinced = []\n",
    "date_convinced = []\n",
    "time_convinced = []\n",
    "for i, file in enumerate([os.listdir('data/tweets/convinced')[j] for j in test_pro]):\n",
    "    print(i, file)\n",
    "    for line in open(''.join(['data/tweets/convinced/', file]), 'r'):\n",
    "        tweets_convinced.append(json.loads(line)['tweet'])\n",
    "        username_convinced.append(json.loads(line)['username'])\n",
    "        name_convinced.append(json.loads(line)['name'])\n",
    "        date_convinced.append(json.loads(line)['date'])\n",
    "        time_convinced.append(json.loads(line)['time'])\n",
    "print(\"\\n\", len(tweets_convinced), \"convinced tweets loaded.\")        \n",
    "        \n",
    "test = pd.DataFrame(list(zip(\n",
    "    tweets_contrarian + tweets_convinced, \n",
    "    [1]*len(tweets_contrarian) + [0]*len(tweets_convinced),\n",
    "    username_contrarian + username_convinced,\n",
    "    name_contrarian + name_convinced,\n",
    "    date_contrarian + date_convinced,\n",
    "    time_contrarian + time_convinced)),\n",
    "                    columns = ['text', 'label', 'username', 'name', 'date', 'time'])\n",
    "\n",
    "print(\"\\n\\nTesting data set with {} tweets created.\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data sets\n",
    "train.to_csv('data/train.csv', index = False, encoding = 'utf-8')\n",
    "valid.to_csv('data/valid.csv', index = False, encoding = 'utf-8')\n",
    "test.to_csv('data/test.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cards-github",
   "language": "python",
   "name": "cards-github"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
