{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Mirjam Nanko\n",
    "## Date Created: 2022-02-22\n",
    "## Email: m.nanko@exeter.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentenceCARDS data preparation<br>\n",
    "#### This script loads and combines the **convinced** (0) and **contrarian** (1) data sourced from various **blogs, twitter, facebook and newspapers**,  disaggregates it into sentences and splits it into **training, validation and testing** data sets. \n",
    "#### To get some measure of external validity, the testing data set is a \"pure\" held out data set that only contains text from \"unseen\" sources, i.e. bloggers, twitter accounts, facebook accounts  and newspaper articles that the classifier was not trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2do \n",
    "# split blogs and newspapers into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(data, account_variable,\n",
    "                           random_test_sample_size = .15, \n",
    "                           valid_sample_size = .25, \n",
    "                           random_seed = 1, random_state = 1, \n",
    "                           shuffle = True):\n",
    "    \"\"\"Split the data into training, validation and testing data set, with the testing \n",
    "    data being completely held-out data by unseen accounts\"\"\"\n",
    "    \n",
    "    # Set a random seed\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Extract unique usernames for each side\n",
    "    contrarian_accounts = data[data[\"label\"]==1][account_variable].unique()\n",
    "    convinced_accounts = data[data[\"label\"]==0][account_variable].unique()\n",
    "    \n",
    "    # Create a randomn sample of users for the testing set for each side\n",
    "    samplesize_contrarian = int(round(random_test_sample_size*len(contrarian_accounts),0))\n",
    "    test_contrarian = random.sample(list(contrarian_accounts), samplesize_contrarian)\n",
    "    samplesize_convinced = int(round(random_test_sample_size*len(convinced_accounts),0))\n",
    "    test_convinced = random.sample(list(convinced_accounts), samplesize_convinced)\n",
    "    \n",
    "    # Separate the testing dataset\n",
    "    trainvalid = pd.concat([data[~data[account_variable].isin(test_contrarian + test_convinced)]])\n",
    "    test = pd.concat([data[data[account_variable].isin(test_contrarian + test_convinced)]])\n",
    "    \n",
    "    # Split training data into training and validation dataset\n",
    "    train, valid = train_test_split(trainvalid, test_size=valid_sample_size, \n",
    "                                    random_state=random_state, shuffle=shuffle)\n",
    "    print(\"Training data set with {} posts created.\".format(len(train)))\n",
    "    print(\"Validation data set with {} posts created.\".format(len(valid)))    \n",
    "    print(\"Testing data set with {} posts created.\".format(len(test)))    \n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "def load_tweets(folderpath, label, verbose = True):\n",
    "    \"\"\"This function loads twitter json files into a pandas dataframe and assigns them a\n",
    "    numeric label.\"\"\"\n",
    "    tweets, username, name, date, time = [[] for _ in range(5)]\n",
    "    for i, file in enumerate(os.listdir(folderpath)):\n",
    "        if verbose == True:\n",
    "            print(i, file)\n",
    "        for line in open('/'.join([folderpath, file]), 'r'):\n",
    "            tweets.append(json.loads(line)['tweet'])\n",
    "            username.append(json.loads(line)['username'])\n",
    "            name.append(json.loads(line)['name'])\n",
    "            date.append(json.loads(line)['date'])\n",
    "            time.append(json.loads(line)['time'])\n",
    "    df = pd.DataFrame(list(zip(tweets, [label]*len(tweets), username, name, date, time)),\n",
    "                    columns = ['text', 'label', 'username', 'name', 'date', 'time'])\n",
    "    if verbose == True:\n",
    "        print(\"\\n\")\n",
    "    return df\n",
    "\n",
    "def load_facebook(folderpath, label, verbose = True):\n",
    "    \"\"\"This function loads facebook json files into a pandas dataframe and assigns them a\n",
    "    numeric label.\"\"\"\n",
    "    text, handle, ID, name, pageAdminTopCountry, pageCategory, date, postUrl, link = [[] for _ in range(9)]\n",
    "    for i, file in enumerate(os.listdir(folderpath)):\n",
    "        if verbose == True:\n",
    "            print(i, file)\n",
    "        posts = json.load(open('/'.join([folderpath, file])))['posts']\n",
    "        for post in posts:\n",
    "            text.append(post.get('message'))\n",
    "            handle.append(post['account'].get('handle'))\n",
    "            ID.append(post['account'].get('id'))\n",
    "            name.append(post['account'].get('name'))\n",
    "            pageAdminTopCountry.append(post['account'].get('pageAdminTopCountry'))\n",
    "            pageCategory.append(post['account'].get('pageCategory'))\n",
    "            date.append(post.get('date'))\n",
    "            postUrl.append(post.get('postUrl'))\n",
    "            link.append(post.get('link'))     \n",
    "    df = pd.DataFrame(list(zip(text, [label]*len(text), handle, ID, name, pageAdminTopCountry, pageCategory, date, postUrl, link)),\n",
    "                    columns = ['text', 'label', 'handle', 'ID', 'name', 'pageAdminTopCountry', 'pageCategory', 'date', 'postUrl', 'link'])\n",
    "    df = df.dropna(subset=['text'])\n",
    "    if verbose == True:\n",
    "        print(\"\\n\")\n",
    "    return df\n",
    "\n",
    "def load_news(folderpath, label, verbose = True):\n",
    "    \"\"\"This function loads factive news csv files into a pandas dataframe and assigns them a\n",
    "    numeric label.\"\"\"\n",
    "    temp = []\n",
    "    for i, file in enumerate(os.listdir(folderpath)):\n",
    "        if verbose == True:\n",
    "            print(i, file)\n",
    "        df_temp = pd.read_csv('/'.join([folderpath, file]), encoding = 'utf-8', lineterminator = '\\n', low_memory = False)\n",
    "        df_temp.drop(columns=['SC', 'CY', 'RE', 'PUB', 'NS', 'CR', 'IPD', 'IPC', 'CT', 'IN', 'RF', 'LA', 'CO', 'ET', 'ED', 'PG'], inplace = True)\n",
    "        df_temp.rename(columns = {'AN':'accession_number','SE': 'section', 'HD': 'headline','WC':'wordcount', 'PD': 'date','SN': 'source',\n",
    "                            'LP': 'lead', 'TD': 'text', 'BY':'author'}, inplace=True)\n",
    "        df_temp[\"ID\"] = df_temp['accession_number'].str.extract(\"Document (.+?)_\\d*\")\n",
    "        df_temp[\"label\"] = label\n",
    "        temp.append(df_temp)\n",
    "    df = pd.concat(temp, axis=0, ignore_index=True)\n",
    "    if verbose == True:\n",
    "        print(\"\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load the blog posts (by blogs, thinktanks and NGOs)---\n",
      "\n",
      " 219535 contrarian blog posts loaded.\n",
      "\n",
      " 110773 convinced blog posts loaded.\n",
      "\n",
      "\n",
      "---Split the data (test data: unseen organisationss)---\n",
      "\n",
      "Training data set with 5462504 posts created.\n",
      "Validation data set with 1820835 posts created.\n",
      "Testing data set with 995101 posts created.\n"
     ]
    }
   ],
   "source": [
    "print('---Load the blog posts (by blogs, thinktanks and NGOs)---')\n",
    "blogs = pd.read_csv('../data/blogs/baby_content_cleanest.csv', encoding = 'utf-8', lineterminator='\\n', \n",
    "                   usecols = ['org', 'date', 'title', 'url', 'org_type', 'org_side', 'text'])\n",
    "blogs[\"label\"] = [1 if i==\"contrarian\" else 0 for i in blogs.org_side]\n",
    "print(\"\\n\", len(blogs[blogs.label == 1]), \"contrarian blog posts loaded.\")\n",
    "print(\"\\n\", len(blogs[blogs.label == 0]), \"convinced blog posts loaded.\\n\\n\")\n",
    "\n",
    "print('---Split the data (test data: unseen organisationss)---\\n')\n",
    "# blogs = blogs.rename(columns={\"org_type\": \"type\"})\n",
    "blogs['type'] = \"blog/thinktank/ngo\"\n",
    "blogs['text'] = blogs['text'].apply(nltk.tokenize.sent_tokenize)\n",
    "blogs = blogs.explode('text')\n",
    "blogs_train, blogs_valid, blogs_test = train_valid_test_split(blogs, \"org\") # Here the held out data is unseen bloggers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load the contrarian tweets---\n",
      "\n",
      "1966699 contrarian tweets loaded.\n",
      "\n",
      "\n",
      "---Load the convinced tweets---\n",
      "\n",
      "1573621 convinced tweets loaded.\n",
      "\n",
      "\n",
      "---Split the data (test data: unseen twitter handles)---\n",
      "\n",
      "Training data set with 3972836 posts created.\n",
      "Validation data set with 1324279 posts created.\n",
      "Testing data set with 577191 posts created.\n"
     ]
    }
   ],
   "source": [
    "print('---Load the contrarian tweets---\\n')\n",
    "twitter_contrarian = load_tweets('../data/twitter/contrarian', label = 1, verbose = False)\n",
    "print(len(twitter_contrarian), \"contrarian tweets loaded.\")\n",
    "\n",
    "print('\\n\\n---Load the convinced tweets---\\n')\n",
    "twitter_convinced = load_tweets('../data/twitter/convinced', label = 0, verbose = False)\n",
    "print(len(twitter_convinced), \"convinced tweets loaded.\\n\\n\")\n",
    "\n",
    "print('---Split the data (test data: unseen twitter handles)---\\n')\n",
    "twitter = pd.concat([twitter_contrarian, twitter_convinced])\n",
    "twitter['type'] = \"twitter\"\n",
    "twitter['text'] = twitter['text'].apply(nltk.tokenize.sent_tokenize)\n",
    "twitter = twitter.explode('text')\n",
    "twitter_train, twitter_valid, twitter_test = train_valid_test_split(twitter, \"username\") # Here the held out data is unseen twitter accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load the contrarian facebook posts---\n",
      "\n",
      "425551 contrarian facebook posts loaded.\n",
      "\n",
      "\n",
      "---Load the convinced facebook posts---\n",
      "\n",
      "527951 convinced facebook posts loaded.\n",
      "\n",
      "\n",
      "---Split the data (test data: unseen facebook handles)---\n",
      "\n",
      "Training data set with 1073363 posts created.\n",
      "Validation data set with 357788 posts created.\n",
      "Testing data set with 190020 posts created.\n"
     ]
    }
   ],
   "source": [
    "print('---Load the contrarian facebook posts---\\n')\n",
    "facebook_contrarian = load_facebook('../data/facebook/contrarian', label = 1, verbose = False)\n",
    "print(len(facebook_contrarian), \"contrarian facebook posts loaded.\")\n",
    "# 449199 contrarian facebook posts loaded.\n",
    "\n",
    "print('\\n\\n---Load the convinced facebook posts---\\n')\n",
    "facebook_convinced = load_facebook('../data/facebook/convinced', label = 0, verbose = False)\n",
    "print(len(facebook_convinced), \"convinced facebook posts loaded.\\n\\n\")\n",
    "\n",
    "print('---Split the data (test data: unseen facebook handles)---\\n')\n",
    "facebook = pd.concat([facebook_contrarian, facebook_convinced])\n",
    "facebook['type'] = \"facebook\"\n",
    "facebook['text'] = facebook['text'].apply(nltk.tokenize.sent_tokenize)\n",
    "facebook = facebook.explode('text')\n",
    "facebook_train, facebook_valid, facebook_test = train_valid_test_split(facebook, \"handle\") # Here the held out data is unseen facebook accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load the contrarian news articles---\n",
      "\n",
      "141954 contrarian news article sentences loaded.\n",
      "\n",
      "\n",
      "---Load the convinced news articles---\n",
      "\n",
      "118665 convinced news article sentences loaded.\n",
      "\n",
      "\n",
      "---Split the data (test data: unseen newspaper articles)---\n",
      "\n",
      "Training data set with 289054 posts created.\n",
      "Validation data set with 96352 posts created.\n",
      "Testing data set with 70831 posts created.\n"
     ]
    }
   ],
   "source": [
    "print('---Load the contrarian news articles---\\n')\n",
    "newspaper_contrarian =  load_news(\"../data/newspapers/contrarian\", label = 1, verbose = False)\n",
    "print(len(newspaper_contrarian), \"contrarian news article sentences loaded.\")\n",
    "\n",
    "print('\\n\\n---Load the convinced news articles---\\n')\n",
    "newspaper_convinced =  load_news(\"../data/newspapers/convinced\", label = 0, verbose = False)\n",
    "print(len(newspaper_convinced), \"convinced news article sentences loaded.\\n\\n\")\n",
    "\n",
    "print('---Split the data (test data: unseen newspaper articles)---\\n')\n",
    "newspapers = pd.concat([newspaper_contrarian, newspaper_convinced])\n",
    "newspapers = newspapers.groupby(['headline', 'wordcount', 'date', 'source', 'lead', 'time_frame', 'ID', 'label'])['text'].apply(' '.join).reset_index()\n",
    "newspapers[newspapers.select_dtypes(['object']).columns] = newspapers[newspapers.select_dtypes(['object']).columns].apply(lambda x: x.str.replace(\"(.\\\\'.{2}.\\\\'.{2})\", \"\"))\n",
    "newspapers['type'] = \"newspaper\"\n",
    "newspapers['text'] = newspapers['text'].apply(nltk.tokenize.sent_tokenize)\n",
    "newspapers = newspapers.explode('text')\n",
    "newspapers_train, newspapers_valid, newspapers_test = train_valid_test_split(newspapers, \"ID\") # Here the held out data is unseen newspaper articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine & export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And Japan sharply walked back its pledge to re...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And if it is told to take this set of numbers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The President's speech to a joint session of C...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>However, rates for those policies will be regu...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Has there ever been a golden age of\\nliberty?</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797752</th>\n",
       "      <td>And then the tears began, drawing an unimpress...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797753</th>\n",
       "      <td>Renewable energy projects tend to be considera...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797754</th>\n",
       "      <td>He has every right to be a skeptic - all scien...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797755</th>\n",
       "      <td>The outbreak of novel coronavirus is an evolvi...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797756</th>\n",
       "      <td>They are trying to agree on the first official...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10797757 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  label  \\\n",
       "0         And Japan sharply walked back its pledge to re...      0   \n",
       "1         And if it is told to take this set of numbers ...      1   \n",
       "2         The President's speech to a joint session of C...      1   \n",
       "3         However, rates for those policies will be regu...      1   \n",
       "4             Has there ever been a golden age of\\nliberty?      1   \n",
       "...                                                     ...    ...   \n",
       "10797752  And then the tears began, drawing an unimpress...      1   \n",
       "10797753  Renewable energy projects tend to be considera...      0   \n",
       "10797754  He has every right to be a skeptic - all scien...      0   \n",
       "10797755  The outbreak of novel coronavirus is an evolvi...      1   \n",
       "10797756  They are trying to agree on the first official...      0   \n",
       "\n",
       "                        type  \n",
       "0         blog/thinktank/ngo  \n",
       "1         blog/thinktank/ngo  \n",
       "2         blog/thinktank/ngo  \n",
       "3         blog/thinktank/ngo  \n",
       "4         blog/thinktank/ngo  \n",
       "...                      ...  \n",
       "10797752           newspaper  \n",
       "10797753           newspaper  \n",
       "10797754           newspaper  \n",
       "10797755           newspaper  \n",
       "10797756           newspaper  \n",
       "\n",
       "[10797757 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([blogs_train[['text', 'label', 'type']],\n",
    "                  twitter_train[['text', 'label', 'type']],\n",
    "                  facebook_train[['text', 'label', 'type']],\n",
    "                  newspapers_train[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image by Jared and Corin.</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>”Andrea Dillon (thell1885@gmail.com) writes fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The ones used for Tropospheric temperature mea...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 4:20, journalist Abby Martin adds her c...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This year, assuming all the countries show and...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599249</th>\n",
       "      <td>The science of climate change tells us we need...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599250</th>\n",
       "      <td>Rises in sea level are expected to be especial...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599251</th>\n",
       "      <td>They turned out along the coast and deep inlan...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599252</th>\n",
       "      <td>Approximately have paid a record US$3 billion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599253</th>\n",
       "      <td>There was just one such nuclear accident - at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3599254 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label  \\\n",
       "0                                Image by Jared and Corin.      1   \n",
       "1        ”Andrea Dillon (thell1885@gmail.com) writes fr...      1   \n",
       "2        The ones used for Tropospheric temperature mea...      0   \n",
       "3        Around 4:20, journalist Abby Martin adds her c...      1   \n",
       "4        This year, assuming all the countries show and...      0   \n",
       "...                                                    ...    ...   \n",
       "3599249  The science of climate change tells us we need...      0   \n",
       "3599250  Rises in sea level are expected to be especial...      0   \n",
       "3599251  They turned out along the coast and deep inlan...      1   \n",
       "3599252  Approximately have paid a record US$3 billion ...      0   \n",
       "3599253  There was just one such nuclear accident - at ...      0   \n",
       "\n",
       "                       type  \n",
       "0        blog/thinktank/ngo  \n",
       "1        blog/thinktank/ngo  \n",
       "2        blog/thinktank/ngo  \n",
       "3        blog/thinktank/ngo  \n",
       "4        blog/thinktank/ngo  \n",
       "...                     ...  \n",
       "3599249           newspaper  \n",
       "3599250           newspaper  \n",
       "3599251           newspaper  \n",
       "3599252           newspaper  \n",
       "3599253           newspaper  \n",
       "\n",
       "[3599254 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = pd.concat([blogs_valid[['text', 'label', 'type']],\n",
    "                  twitter_valid[['text', 'label', 'type']],\n",
    "                  facebook_valid[['text', 'label', 'type']],\n",
    "                  newspapers_valid[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been fortunate to have met some people wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One such person was Dr. Paul Epstein, a physic...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dr. Epstein, associate director of the Center ...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People don't just want to know how climate cha...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They want to know what's at stake for them.</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833138</th>\n",
       "      <td>Princeton philosopher Harry Frankfurt called b...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833139</th>\n",
       "      <td>Everyone knows this  (but) we have no clear un...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833140</th>\n",
       "      <td>Its become much worse since then, given a clim...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833141</th>\n",
       "      <td>In the 19th century Thorstein Veblen coined th...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833142</th>\n",
       "      <td>Its successor, conspicuous compassion, is a lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1833143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label  \\\n",
       "0        I've been fortunate to have met some people wi...      0   \n",
       "1        One such person was Dr. Paul Epstein, a physic...      0   \n",
       "2        Dr. Epstein, associate director of the Center ...      0   \n",
       "3        People don't just want to know how climate cha...      0   \n",
       "4              They want to know what's at stake for them.      0   \n",
       "...                                                    ...    ...   \n",
       "1833138  Princeton philosopher Harry Frankfurt called b...      1   \n",
       "1833139  Everyone knows this  (but) we have no clear un...      1   \n",
       "1833140  Its become much worse since then, given a clim...      1   \n",
       "1833141  In the 19th century Thorstein Veblen coined th...      1   \n",
       "1833142  Its successor, conspicuous compassion, is a lo...      1   \n",
       "\n",
       "                       type  \n",
       "0        blog/thinktank/ngo  \n",
       "1        blog/thinktank/ngo  \n",
       "2        blog/thinktank/ngo  \n",
       "3        blog/thinktank/ngo  \n",
       "4        blog/thinktank/ngo  \n",
       "...                     ...  \n",
       "1833138           newspaper  \n",
       "1833139           newspaper  \n",
       "1833140           newspaper  \n",
       "1833141           newspaper  \n",
       "1833142           newspaper  \n",
       "\n",
       "[1833143 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.concat([blogs_test[['text', 'label', 'type']],\n",
    "                  twitter_test[['text', 'label', 'type']],\n",
    "                  facebook_test[['text', 'label', 'type']],\n",
    "                  newspapers_test[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blog/thinktank/ngo    0.550564\n",
       "twitter               0.355244\n",
       "facebook              0.074529\n",
       "newspaper             0.019663\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['type', 'label']][train.label == 1].type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blog/thinktank/ngo    0.438624\n",
       "twitter               0.387037\n",
       "facebook              0.136867\n",
       "newspaper             0.037471\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['type', 'label']][train.label == 0].type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.600932\n",
       "0    0.399068\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data sets\n",
    "train.to_csv('../data/sentenceCARDStrain.csv', index = False, encoding = 'utf-8')\n",
    "valid.to_csv('../data/sentenceCARDSvalid.csv', index = False, encoding = 'utf-8')\n",
    "test.to_csv('../data/sentenceCARDStest.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
