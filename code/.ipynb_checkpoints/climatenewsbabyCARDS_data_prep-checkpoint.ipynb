{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Mirjam Nanko\n",
    "## Date Created: 2022-02-24\n",
    "## Email: m.nanko@exeter.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# climatearticlebabyCARDS data preparation<br>\n",
    "#### This script loads and combines the **convinced** (0) and **contrarian** (1) data sourced from various **blogs and newspapers**, subsets the data to documents mentioning the term *climate* or *global warming* and splits it into **training, validation and testing** data sets. \n",
    "#### To get some measure of external validity, the testing data set is a \"pure\" held out data set that only contains text from \"unseen\" sources, i.e. bloggers and newspaper articles that the classifier was not trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0986f84c3aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(data, account_variable,\n",
    "                           random_test_sample_size = .15, \n",
    "                           valid_sample_size = .25, \n",
    "                           random_seed = 1, random_state = 1, \n",
    "                           shuffle = True):\n",
    "    \"\"\"Split the data into training, validation and testing data set, with the testing \n",
    "    data being completely held-out data by unseen accounts\"\"\"\n",
    "    \n",
    "    # Set a random seed\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Extract unique usernames for each side\n",
    "    contrarian_accounts = data[data[\"label\"]==1][account_variable].unique()\n",
    "    convinced_accounts = data[data[\"label\"]==0][account_variable].unique()\n",
    "    \n",
    "    # Create a randomn sample of users for the testing set for each side\n",
    "    samplesize_contrarian = int(round(random_test_sample_size*len(contrarian_accounts),0))\n",
    "    test_contrarian = random.sample(list(contrarian_accounts), samplesize_contrarian)\n",
    "    samplesize_convinced = int(round(random_test_sample_size*len(convinced_accounts),0))\n",
    "    test_convinced = random.sample(list(convinced_accounts), samplesize_convinced)\n",
    "    \n",
    "    # Separate the testing dataset\n",
    "    trainvalid = pd.concat([data[~data[account_variable].isin(test_contrarian + test_convinced)]])\n",
    "    test = pd.concat([data[data[account_variable].isin(test_contrarian + test_convinced)]])\n",
    "    \n",
    "    # Split training data into training and validation dataset\n",
    "    train, valid = train_test_split(trainvalid, test_size=valid_sample_size, \n",
    "                                    random_state=random_state, shuffle=shuffle)\n",
    "    print(\"Training data set with {} posts created.\".format(len(train)))\n",
    "    print(\"Validation data set with {} posts created.\".format(len(valid)))    \n",
    "    print(\"Testing data set with {} posts created.\".format(len(test)))    \n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "def load_news(folderpath, label, verbose = True):\n",
    "    \"\"\"This function loads factive news csv files into a pandas dataframe and assigns them a\n",
    "    numeric label.\"\"\"\n",
    "    temp = []\n",
    "    for i, file in enumerate(os.listdir(folderpath)):\n",
    "        if verbose == True:\n",
    "            print(i, file)\n",
    "        df_temp = pd.read_csv('/'.join([folderpath, file]), encoding = 'utf-8', lineterminator = '\\n', low_memory = False)\n",
    "        df_temp.drop(columns=['SC', 'CY', 'RE', 'PUB', 'NS', 'CR', 'IPD', 'IPC', 'CT', 'IN', 'RF', 'LA', 'CO', 'ET', 'ED', 'PG'], inplace = True)\n",
    "        df_temp.rename(columns = {'AN':'accession_number','SE': 'section', 'HD': 'headline','WC':'wordcount', 'PD': 'date','SN': 'source',\n",
    "                            'LP': 'lead', 'TD': 'text', 'BY':'author'}, inplace=True)\n",
    "        df_temp[\"ID\"] = df_temp['accession_number'].str.extract(\"Document (.+?)_\\d*\")\n",
    "        df_temp[\"label\"] = label\n",
    "        temp.append(df_temp)\n",
    "    df = pd.concat(temp, axis=0, ignore_index=True)\n",
    "    if verbose == True:\n",
    "        print(\"\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the factiva search terms\n",
    "factiva = \"|\".join([\"climate change\", \"global warming\", \"greenhouse gas\", \"CO2\", \"carbon dioxide\", \"extreme weather\", \"fossil fuel.{1}\",\n",
    "                    \"renewable energy\", \"carbon footprint\", \"carbon price\", \"carbon pricing\", \"carbon tax\", \"climate change and trade\",\n",
    "                    \"climate change and cap\", \"carbon storage\", \"IPCC\", \"Paris Agreement\", \"sustainable energy\", \"Green New Deal\",\n",
    "                    \"energy efficiency\", \"green energy\", \"nuclear power\", \"solar power\", \"wind power\", \"eco-friendly\", \"GHG\", \"green washing\",\n",
    "                     \"(?:^(?=.*(?:COP12|COP13|COP14|COP15|COP16|COP17|COP18|COP19|COP20|COP21|COP22|COP23|COP24|COP25|COP26))(?=.*climate))\",\n",
    "                    \"(?:^(?=.*(?:solution|policy|electr.{1}|energy|ozone|temperature|aerosols|UNFCCC|INDC|methane|G7 summit|sustainab.{1}))(?=.*(?:climate change|global warming|climate)))\"]).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blogs, thinktanks and NGOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Load the blog posts (by blogs, thinktanks and NGOs)---\n",
      "\n",
      " 219535 contrarian blog posts loaded.\n",
      "\n",
      " 110773 convinced blog posts loaded.\n",
      "\n",
      "\n",
      "---Subset the posts matching the factiva search terms---\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print('---Load the blog posts (by blogs, thinktanks and NGOs)---')\n",
    "blogs = pd.read_csv('../data/blogs/baby_content_cleanest.csv', encoding = 'utf-8', lineterminator='\\n', \n",
    "                   usecols = ['org', 'date', 'title', 'url', 'org_type', 'org_side', 'text'])\n",
    "blogs[\"label\"] = [1 if i==\"contrarian\" else 0 for i in blogs.org_side]\n",
    "print(\"\\n\", len(blogs[blogs.label == 1]), \"contrarian blog posts loaded.\")\n",
    "print(\"\\n\", len(blogs[blogs.label == 0]), \"convinced blog posts loaded.\\n\\n\")\n",
    "\n",
    "# Subset the data to articles matching the factiva search terms\n",
    "print('---Subset the posts matching the factiva search terms---')\n",
    "blogs = blogs.rename(columns={\"org_type\": \"type\"})\n",
    "blogs['type'] = \"blog/thinktank/ngo\"\n",
    "blogs = blogs[blogs['text'].str.lower().str.contains(factiva, regex=True)]\n",
    "print(\"\\n\", len(blogs[blogs.label == 1]), \"contrarian blog posts remain.\")\n",
    "print(\"\\n\", len(blogs[blogs.label == 0]), \"convinced blog posts remain.\\n\\n\")\n",
    "\n",
    "# Split the data \n",
    "print('---Split the data (test data: unseen organisationss)---\\n')\n",
    "blogs_train, blogs_valid, blogs_test = train_valid_test_split(blogs, \"org\") # Here the held out data is unseen bloggers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "print('---Load the contrarian news articles---\\n')\n",
    "newspaper_contrarian =  load_news(\"../data/newspapers/contrarian\", label = 1, verbose = False)\n",
    "print(len(newspaper_contrarian), \"contrarian news article sentences loaded.\")\n",
    "print('\\n\\n---Load the convinced news articles---\\n')\n",
    "newspaper_convinced =  load_news(\"../data/newspapers/convinced\", label = 0, verbose = False)\n",
    "print(len(newspaper_convinced), \"convinced news article sentences loaded.\\n\\n\")\n",
    "\n",
    "# Subset the data to articles matching the factiva search terms\n",
    "print('---Subset the posts matching the factiva search terms---')\n",
    "newspapers = pd.concat([newspaper_contrarian, newspaper_convinced])\n",
    "newspapers = newspapers.groupby(['headline', 'wordcount', 'date', 'source', 'lead', 'time_frame', 'ID', 'label'])['text'].apply(' '.join).reset_index()\n",
    "newspapers[newspapers.select_dtypes(['object']).columns] = newspapers[newspapers.select_dtypes(['object']).columns].apply(lambda x: x.str.replace(\"(.\\\\'.{2}.\\\\'.{2})\", \"\"))\n",
    "newspapers['type'] = \"newspaper\"\n",
    "newspapers = newspapers[newspapers['text'].str.lower().str.contains(factiva, regex=True)]\n",
    "print(len(newspaper_contrarian), \"contrarian news article sentences remain.\")\n",
    "print(len(newspaper_convinced), \"convinced news article sentences remain.\\n\\n\")\n",
    "\n",
    "# Split the data\n",
    "print('---Split the data (test data: unseen newspaper articles)---\\n')\n",
    "newspapers_train, newspapers_valid, newspapers_test = train_valid_test_split(newspapers, \"ID\") # Here the held out data is unseen newspaper articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine & export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The November issue of Environment &amp; Climate Ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This article in the Guardian offers us quite a...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The University of East Anglia who were at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (April 29, 2008) – Deputy Premier o...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate Change Weekly #83New data from the Nat...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379865</th>\n",
       "      <td>The three countries have been offered almost a...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379866</th>\n",
       "      <td>The US bank JP Morgan Chase , whose economists...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379867</th>\n",
       "      <td>The fund, previously known as the Sustainable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379868</th>\n",
       "      <td>Human ingenuity has created a way of living th...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379869</th>\n",
       "      <td>The local sharemarket finished firmly lower on...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>379870 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       The November issue of Environment & Climate Ne...      1   \n",
       "1       This article in the Guardian offers us quite a...      1   \n",
       "2       The University of East Anglia who were at the ...      1   \n",
       "3       WASHINGTON (April 29, 2008) – Deputy Premier o...      0   \n",
       "4       Climate Change Weekly #83New data from the Nat...      1   \n",
       "...                                                   ...    ...   \n",
       "379865  The three countries have been offered almost a...      0   \n",
       "379866  The US bank JP Morgan Chase , whose economists...      0   \n",
       "379867  The fund, previously known as the Sustainable ...      1   \n",
       "379868  Human ingenuity has created a way of living th...      0   \n",
       "379869  The local sharemarket finished firmly lower on...      1   \n",
       "\n",
       "                      type  \n",
       "0       blog/thinktank/ngo  \n",
       "1       blog/thinktank/ngo  \n",
       "2       blog/thinktank/ngo  \n",
       "3       blog/thinktank/ngo  \n",
       "4       blog/thinktank/ngo  \n",
       "...                    ...  \n",
       "379865           newspaper  \n",
       "379866           newspaper  \n",
       "379867           newspaper  \n",
       "379868           newspaper  \n",
       "379869           newspaper  \n",
       "\n",
       "[379870 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([blogs_train[['text', 'label', 'type']],\n",
    "                  newspapers_train[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From the Wall Street Journal : \\n  The letter ...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oregon's Prius (hybrid) tax credit could go as...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There's an African proverb which says \"when yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The News | News   Around 300 coal miners and p...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>March 4th was the birthday or deathday of at l...</td>\n",
       "      <td>1</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126621</th>\n",
       "      <td>Back in 2006, when Congress passed a $2,000 cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126622</th>\n",
       "      <td>The study, which was published in the Nature t...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126623</th>\n",
       "      <td>Research funding is closely related to publish...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126624</th>\n",
       "      <td>Once the rod is spent, it is returned for proc...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126625</th>\n",
       "      <td>The power giant plans to launch a study into h...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       From the Wall Street Journal : \\n  The letter ...      1   \n",
       "1       Oregon's Prius (hybrid) tax credit could go as...      1   \n",
       "2       There's an African proverb which says \"when yo...      0   \n",
       "3       The News | News   Around 300 coal miners and p...      1   \n",
       "4       March 4th was the birthday or deathday of at l...      1   \n",
       "...                                                   ...    ...   \n",
       "126621  Back in 2006, when Congress passed a $2,000 cr...      0   \n",
       "126622  The study, which was published in the Nature t...      0   \n",
       "126623  Research funding is closely related to publish...      1   \n",
       "126624  Once the rod is spent, it is returned for proc...      1   \n",
       "126625  The power giant plans to launch a study into h...      1   \n",
       "\n",
       "                      type  \n",
       "0       blog/thinktank/ngo  \n",
       "1       blog/thinktank/ngo  \n",
       "2       blog/thinktank/ngo  \n",
       "3       blog/thinktank/ngo  \n",
       "4       blog/thinktank/ngo  \n",
       "...                    ...  \n",
       "126621           newspaper  \n",
       "126622           newspaper  \n",
       "126623           newspaper  \n",
       "126624           newspaper  \n",
       "126625           newspaper  \n",
       "\n",
       "[126626 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = pd.concat([blogs_valid[['text', 'label', 'type']],\n",
    "                  twitter_valid[['text', 'label', 'type']],\n",
    "                  facebook_valid[['text', 'label', 'type']],\n",
    "                  newspapers_valid[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been fortunate to have met some people wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You may have heard that a draft of the Nationa...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate change will make the drought and flood...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you own stocks or have money in a retiremen...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investors and the global environment are at ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>blog/thinktank/ngo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61137</th>\n",
       "      <td>The ACCR report raises the bar for super funds...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61138</th>\n",
       "      <td>And then theres the large, black Puma knapsack...</td>\n",
       "      <td>0</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61139</th>\n",
       "      <td>A major upgrade of the Vales Point coal-fired ...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61140</th>\n",
       "      <td>Jane Caro and Diana are only contemporaries in...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61141</th>\n",
       "      <td>You need to work with the country. It is going...</td>\n",
       "      <td>1</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      I've been fortunate to have met some people wi...      0   \n",
       "1      You may have heard that a draft of the Nationa...      0   \n",
       "2      Climate change will make the drought and flood...      0   \n",
       "3      If you own stocks or have money in a retiremen...      0   \n",
       "4      Investors and the global environment are at ri...      0   \n",
       "...                                                  ...    ...   \n",
       "61137  The ACCR report raises the bar for super funds...      1   \n",
       "61138  And then theres the large, black Puma knapsack...      0   \n",
       "61139  A major upgrade of the Vales Point coal-fired ...      1   \n",
       "61140  Jane Caro and Diana are only contemporaries in...      1   \n",
       "61141  You need to work with the country. It is going...      1   \n",
       "\n",
       "                     type  \n",
       "0      blog/thinktank/ngo  \n",
       "1      blog/thinktank/ngo  \n",
       "2      blog/thinktank/ngo  \n",
       "3      blog/thinktank/ngo  \n",
       "4      blog/thinktank/ngo  \n",
       "...                   ...  \n",
       "61137           newspaper  \n",
       "61138           newspaper  \n",
       "61139           newspaper  \n",
       "61140           newspaper  \n",
       "61141           newspaper  \n",
       "\n",
       "[61142 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.concat([blogs_test[['text', 'label', 'type']],\n",
    "                  twitter_test[['text', 'label', 'type']],\n",
    "                  facebook_test[['text', 'label', 'type']],\n",
    "                  newspapers_test[['text', 'label', 'type']]]).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blog/thinktank/ngo    0.441389\n",
       "twitter               0.438138\n",
       "facebook              0.110115\n",
       "newspaper             0.010358\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['type', 'label']][train.label == 1].type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twitter               0.678080\n",
       "blog/thinktank/ngo    0.173496\n",
       "facebook              0.133583\n",
       "newspaper             0.014841\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['type', 'label']][train.label == 0].type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.552181\n",
       "1    0.447819\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data sets\n",
    "train.to_csv('../data/climatebabyCARDStrain.csv', index = False, encoding = 'utf-8')\n",
    "valid.to_csv('../data/climatebabyCARDSvalid.csv', index = False, encoding = 'utf-8')\n",
    "test.to_csv('../data/climatebabyCARDStest.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
